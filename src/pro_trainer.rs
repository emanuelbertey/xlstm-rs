#![recursion_limit = "256"]

/*!
xLSTM Pro Trainer - High Performance Data Streaming
    
This tool is designed to train xLSTM models on large datasets (directories, 
multiple file formats, and gigabyte-sized files) without exhausting RAM.
*/

use burn::grad_clipping::GradientClippingConfig;
use burn::optim::decay::WeightDecayConfig;
use burn::{
    module::AutodiffModule,
    module::Module,
    optim::AdamConfig,
    record::{CompactRecorder, Recorder},
    tensor::{activation::softmax, Tensor, backend::{AutodiffBackend, Backend}},
    nn::loss::CrossEntropyLossConfig,
};
use burn::tensor::TensorData;
use burn_autodiff::Autodiff;
use std::error::Error;
use std::fs;
use std::io::{self, Write, Read, BufReader};
use std::path::{Path, PathBuf};
use std::time::Instant;
use burn_ndarray::NdArray;
use tokenizers::pre_tokenizers::byte_level::ByteLevel;
use tokenizers::decoders::byte_level::ByteLevel as ByteLevelDecoder;
use tokenizers::models::bpe::{BpeTrainerBuilder, BPE};
use tokenizers::tokenizer::Tokenizer as HFTokenizer;
use tokenizers::models::TrainerWrapper;

use xlstm::{LearningRateConfig, LstmType, XLstm, XLstmconfig};

type MyBackend = Autodiff<NdArray<f32>>;

/// Professional BPE Tokenizer
pub struct Tokenizer {
    tokenizer: HFTokenizer,
}

impl Tokenizer {
    pub fn from_text(text: &str, vocab_size: usize) -> Result<Self, Box<dyn Error>> {
        let model = BPE::builder()
            .byte_fallback(true)
            .build()
            .map_err(|e| format!("Error building BPE: {}", e))?;
            
        let mut tokenizer = HFTokenizer::new(model);
        // ByteLevel preserva exactamente todos los caracteres (espacios, tabs, saltos)
        tokenizer.with_pre_tokenizer(Some(ByteLevel::default()));
        tokenizer.with_decoder(Some(ByteLevelDecoder::default()));

        let trainer = BpeTrainerBuilder::default()
            .show_progress(true)
            .vocab_size(vocab_size)
            .min_frequency(2)
            .build();

        let mut trainer_wrapper = TrainerWrapper::from(trainer);
        let temp_file = "temp_train_pro.txt";
        fs::write(temp_file, text)?;
        tokenizer.train_from_files(&mut trainer_wrapper, vec![temp_file.to_string()])
            .map_err(|e| format!("Error in training: {}", e))?;
        fs::remove_file(temp_file)?;

        Ok(Self { tokenizer })
    }

    pub fn save(&self, path: &str) -> Result<(), Box<dyn Error>> {
        self.tokenizer.save(path, true).map_err(|e| format!("Error saving: {}", e))?;
        Ok(())
    }

    pub fn load(path: &str) -> Result<Self, Box<dyn Error>> {
        let mut tokenizer = HFTokenizer::from_file(path).map_err(|e| format!("Error loading: {}", e))?;
        tokenizer.with_decoder(Some(ByteLevelDecoder::default()));
        Ok(Self { tokenizer })
    }

    pub fn encode(&self, text: &str) -> Vec<usize> {
        let encoding = self.tokenizer.encode(text, false).unwrap();
        encoding.get_ids().iter().map(|&id| id as usize).collect()
    }

    pub fn decode(&self, indices: &[usize]) -> String {
        let u32_indices: Vec<u32> = indices.iter().map(|&idx| idx as u32).collect();
        self.tokenizer.decode(&u32_indices, true).unwrap()
    }

    pub fn vocab_size(&self) -> usize {
        self.tokenizer.get_vocab_size(true)
    }

    pub fn id_to_token(&self, id: usize) -> Option<String> {
        self.tokenizer.id_to_token(id as u32)
    }
}

/// Recursively find files with specific extensions
fn find_files(dir: &Path, extensions: &[&str]) -> Vec<PathBuf> {
    let mut files = Vec::new();
    if let Ok(entries) = fs::read_dir(dir) {
        for entry in entries.flatten() {
            let path = entry.path();
            if path.is_dir() {
                files.extend(find_files(&path, extensions));
            } else if let Some(ext) = path.extension().and_then(|e| e.to_str()) {
                if extensions.contains(&ext) {
                    files.push(path);
                }
            }
        }
    }
    files
}

/// Iterator for large files that yields fragments (buffers) to save RAM
struct FileFragmentIterator {
    reader: BufReader<fs::File>,
    buffer_size: usize,
    finished: bool,
}

impl FileFragmentIterator {
    fn new(path: &Path, buffer_size_mb: usize) -> io::Result<Self> {
        let file = fs::File::open(path)?;
        Ok(Self {
            reader: BufReader::new(file),
            buffer_size: buffer_size_mb * 1024 * 1024,
            finished: false,
        })
    }
}

impl Iterator for FileFragmentIterator {
    type Item = String;

    fn next(&mut self) -> Option<Self::Item> {
        if self.finished { return None; }

        let mut buffer = vec![0u8; self.buffer_size];
        let mut total_read = 0;

        while total_read < self.buffer_size {
            match self.reader.read(&mut buffer[total_read..]) {
                Ok(0) => { self.finished = true; break; }
                Ok(n) => total_read += n,
                Err(ref e) if e.kind() == io::ErrorKind::Interrupted => continue,
                Err(_) => { self.finished = true; break; }
            }
        }

        if total_read == 0 { return None; }
        buffer.truncate(total_read);

        // Ensure we don't split a UTF-8 character
        while !buffer.is_empty() && String::from_utf8(buffer.clone()).is_err() {
            buffer.pop();
        }

        if buffer.is_empty() { return None; }
        String::from_utf8(buffer).ok()
    }
}

fn create_batch<B: AutodiffBackend>(
    tokens: &[usize],
    start_idx: usize,
    batch_size: usize,
    seq_length: usize,
    stride: usize,
    vocab_size: usize,
    device: &B::Device,
) -> (Tensor<B, 3>, Tensor<B, 2, burn::tensor::Int>) {
    let mut x_indices = Vec::with_capacity(batch_size * seq_length);
    let mut y_indices = Vec::with_capacity(batch_size * seq_length);

    for i in 0..batch_size {
        let current_start = start_idx + i * stride;
        
        // RELLENO (PADDING): Si no hay suficientes tokens, rellenamos con ceros (o el Ãºltimo token)
        for j in 0..seq_length {
            let idx_x = if current_start + j < tokens.len() { tokens[current_start + j] } else { 0 };
            let idx_y = if current_start + j + 1 < tokens.len() { tokens[current_start + j + 1] } else { 0 };
            
            x_indices.push(idx_x as i64);
            y_indices.push(idx_y as i64);
        }
    }

    let eye = Tensor::<B::InnerBackend, 2>::eye(vocab_size, device);
    let indices_inner = Tensor::<B::InnerBackend, 1, burn::tensor::Int>::from_data(
        TensorData::new(x_indices, [batch_size * seq_length]),
        device,
    );

    let x = Tensor::<B, 3>::from_inner(
        eye.select(0, indices_inner).reshape([batch_size, seq_length, vocab_size])
    );
    
    let y = Tensor::<B, 2, burn::tensor::Int>::from_data(
        TensorData::new(y_indices, [batch_size, seq_length]),
        device,
    );

    (x, y)
}

fn sample_from_logits<B: Backend>(
    logits: Tensor<B, 2>, 
    temperature: f32,
    top_k: usize,
    top_p: f32
) -> usize
where
    <B as Backend>::FloatElem: num_traits::ToPrimitive,
{
    let probs = softmax(logits, 1);
    let mut probs_vec: Vec<(usize, f32)> = probs.to_data()
        .as_slice::<<B as Backend>::FloatElem>()
        .unwrap()
        .iter()
        .enumerate()
        .map(|(i, &x)| (i, num_traits::ToPrimitive::to_f32(&x).unwrap_or(0.0)))
        .collect();

    probs_vec.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
    
    let k = top_k.min(probs_vec.len()).max(1);
    let mut filtered_probs: Vec<(usize, f32)> = Vec::with_capacity(k);
    
    let mut cumulative_prob = 0.0;
    for (i, p) in probs_vec.into_iter() {
        filtered_probs.push((i, p));
        cumulative_prob += p;
        if filtered_probs.len() >= k || cumulative_prob >= top_p {
            break;
        }
    }

    let indices: Vec<usize> = filtered_probs.iter().map(|(i, _)| *i).collect();
    let mut weights: Vec<f32> = filtered_probs.iter().map(|(_, p)| *p).collect();

    if temperature <= 1e-6 { return indices[0]; }

    for p in weights.iter_mut() {
        *p = (p.max(1e-10).ln() / temperature).exp();
    }

    let sum: f32 = weights.iter().sum();
    use rand::Rng;
    let mut rng = rand::rng(); 
    let sample: f32 = rng.random::<f32>() * sum; 
    let mut cumulative = 0.0;

    for (i, &p) in weights.iter().enumerate() {
        cumulative += p;
        if sample <= cumulative { return indices[i]; }
    }

    indices[0]
}

fn generate_text<B: Backend>(
    model: &XLstm<B>,
    tokenizer: &Tokenizer,
    seed_text: &str,
    length: usize,
    vocab_size: usize,
    device: &B::Device,
) -> String
where
    <B as Backend>::FloatElem: num_traits::ToPrimitive + num_traits::FromPrimitive,
{
    let mut generated_ids = tokenizer.encode(seed_text);
    let seed_tokens = generated_ids.clone();
    if seed_tokens.is_empty() { return seed_text.to_string(); }

    let eye = Tensor::<B, 2>::eye(vocab_size, device);
    let mut current_state = None; 
    let mut current_tokens = seed_tokens.clone();

    for i in 0..length {
        let tokens_to_process = if i == 0 { current_tokens.clone() } else { vec![*current_tokens.last().unwrap()] };
        let seq_len = tokens_to_process.len();
        let indices = Tensor::<B, 1, burn::tensor::Int>::from_data(
            TensorData::new(tokens_to_process.iter().map(|&t| t as i64).collect(), [seq_len]),
            device,
        );

        let input = eye.clone().select(0, indices).reshape([1, seq_len, vocab_size]);
        let (output, next_state) = model.forward(input, current_state);
        current_state = Some(next_state);

        let dims = output.dims();
        let last_logits = output.slice([0..1, (dims[1] - 1)..dims[1], 0..dims[2]]).reshape([1, dims[2]]);
        
        let next_token = sample_from_logits(last_logits, 0.7, 40, 0.9);

        current_tokens.push(next_token);
        generated_ids.push(next_token);
    }

    tokenizer.decode(&generated_ids[seed_tokens.len()..])
}


fn main() -> Result<(), Box<dyn Error>> {
    println!("xLSTM Pro Trainer - AutomatizaciÃ³n de Directorios");
    println!("================================================\n");

    let args: Vec<String> = std::env::args().collect();
    if args.len() < 2 {
        eprintln!("Uso: cargo run --bin pro_trainer -- <archivo_o_carpeta>");
        std::process::exit(1);
    }

    let input_path = Path::new(&args[1]);
    let tokenizer_path = "tokenizer_pro.json";
    let model_path = "xlstm_pro_model";
    let extensions = ["txt", "gd", "html" , "cpp", "c", "py", "h", "md"];

    let all_files = if input_path.is_dir() {
        println!("Explorando directorio: {:?}", input_path);
        find_files(input_path, &extensions)
    } else {
        vec![input_path.to_path_buf()]
    };

    if all_files.is_empty() {
        return Err("No se encontraron archivos vÃ¡lidos (.txt, .gd, .html)".into());
    }
    println!("Archivos encontrados: {}", all_files.len());

    let target_vocab_size = 2048;
    let tokenizer = if Path::new(tokenizer_path).exists() {
        Tokenizer::load(tokenizer_path)?
    } else {
        println!("Entrenando nuevo tokenizador con muestras...");
        let mut sample_text = String::new();
        for file_path in all_files.iter().take(5) {
            if let Ok(content) = fs::read_to_string(file_path) {
                let short_content: String = content.chars().take(20000).collect();
                sample_text.push_str(&short_content);
            }
        }
        let tokenizer = Tokenizer::from_text(&sample_text, target_vocab_size)?;
        tokenizer.save(tokenizer_path)?;
        tokenizer
    };

    let vocab_size = tokenizer.vocab_size();
    let hidden_size = 320;
    let num_layers = 1;
    let num_blocks = 4;
    let seq_length = 256;
    let batch_size = 16;
    let stride = 256;
    let num_epochs = 50;

    let device = Default::default();
    let dropout = 0.1;
    let num_heads = 4;

    let config = XLstmconfig::new(vocab_size, hidden_size, num_layers, num_blocks, vocab_size)
        .with_dropout(dropout)
        .with_num_heads(num_heads)
        .with_lstm_type(LstmType::MLSTM)
        .with_use_projection(true);   

    let model_file = format!("{}.mpk", model_path);
    let mut model = if Path::new(&model_file).exists() {
        println!("Cargando modelo previo '{}'...", model_file);
        let recorder = CompactRecorder::new();
        let record = recorder.load(model_file.into(), &device)
            .map_err(|e| format!("Error al cargar modelo: {}", e))?;
        config.init::<MyBackend>(&device).load_record(record)
    } else {
        println!("No se encontrÃ³ modelo previo. Iniciando desde cero.");
        config.init::<MyBackend>(&device)
    };

    println!("\nÂ¿QuÃ© deseas hacer?");
    println!("1. Entrenar (continuar o empezar)");
    println!("2. Inferir (generar texto interactivo)");
    print!("SelecciÃ³n > ");
    io::stdout().flush()?;
    let mut choice = String::new();
    io::stdin().read_line(&mut choice)?;

    if choice.trim() == "2" {
        println!("\nModo Inferencia Chat");
        println!("====================");
        println!("Comandos:");
        println!("  - Escribe tu semilla para generar");
        println!("  - 'len <n>' para cambiar longitud (ej: len 500)");
        println!("  - 'salir' para finalizar\n");

        let mut current_len = 200;
        loop {
            print!("Semilla (len: {}) > ", current_len);
            io::stdout().flush()?;
            let mut input = String::new();
            io::stdin().read_line(&mut input)?;
            let input = input.trim();
            
            if input.is_empty() { continue; }
            if input == "salir" { break; }

            if input.starts_with("len ") {
                if let Ok(new_len) = input[4..].trim().parse::<usize>() {
                    current_len = new_len;
                    println!("  -> Longitud cambiada a: {}", current_len);
                    continue;
                }
            }

            println!("Generando...");
            let generated = generate_text(&model.valid(), &tokenizer, input, current_len, vocab_size, &device);
            if generated.is_empty() {
                println!("  âš ï¸ El modelo generÃ³ texto vacÃ­o.");
            }
            // Limpieza solo visual para el chat
            let clean_text = generated
                .replace('â–', " ")
                .replace('Ä ', " ")
                .replace('ÄŠ', "\n")
                .replace('â†²', "\n")
                .replace('â£', " ");
            
            // Si el texto parece vacÃ­o pero hay tokens, es que son solo espacios/control
            if clean_text.trim().is_empty() && !generated.is_empty() {
                println!("â”‚ [Contenido: {} espacios/tabs]", generated.len());
            }
            
            // The original `let clean_text = generated;` was redundant after the new `clean_text` definition.
            // It's removed to avoid shadowing and use the newly cleaned text.

            println!("\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®");
            println!("â”‚ TEXTO GENERADO:");
            println!("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
            
            for line in clean_text.lines() {
                let mut current_line = line.to_string();
                if current_line.is_empty() {
                    println!("â”‚");
                    continue;
                }
                while current_line.len() > 58 {
                    let chunk: String = current_line.chars().take(58).collect();
                    println!("â”‚ {}", chunk);
                    current_line = current_line.chars().skip(58).collect();
                }
                println!("â”‚ {}", current_line);
            }
            println!("â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n");
        }
        return Ok(());
    }

    let mut optim = AdamConfig::new()
        .with_beta_1(0.9)
        .with_beta_2(0.999)
        .with_epsilon(1e-8)
        .with_weight_decay(Some(WeightDecayConfig::new(1e-4)))
        .with_grad_clipping(Some(GradientClippingConfig::Norm(1.0)))
        .init();
    let loss_fn = CrossEntropyLossConfig::new().init(&device);
    let lr_config = LearningRateConfig::per_block_type(1e-3, 1e-3, 1e-3, 1e-3);

    println!("\nIniciando entrenamiento por fragmentos...");

    for epoch in 0..num_epochs {
        println!("\n--- Ã‰poca {}/{} ---", epoch + 1, num_epochs);
        let mut total_loss = 0.0f32;
        let mut num_batches_processed = 0;

        let mut last_save_time = Instant::now();
        let mut token_buffer = Vec::new();
        let tokens_per_batch = batch_size * stride;
        let tokens_needed_for_batch = tokens_per_batch + seq_length; 

        for (f_idx, file_path) in all_files.iter().enumerate() {
            // Solo imprimimos si el archivo es relevante o para dar feedback de progreso
            if f_idx % 10 == 0 || all_files.len() < 100 {
                print!("\r  [{}/{}] Analizando: {:<40}", f_idx + 1, all_files.len(), file_path.file_name().unwrap_or_default().to_string_lossy());
                io::stdout().flush().unwrap();
            }

            let fragments = FileFragmentIterator::new(file_path, 64).map_err(|e| e.to_string())?;
            
            for (_fr_idx, fragment) in fragments.enumerate() {
                token_buffer.extend(tokenizer.encode(&fragment));

                // Procesamos mientras el bÃºfer tenga suficientes tokens para al menos un batch completo
                while token_buffer.len() >= tokens_needed_for_batch + 1 {
                    let batch_start_time = Instant::now();
                    
                    let (input, target) = create_batch::<MyBackend>(
                        &token_buffer, 0, batch_size, seq_length, stride, vocab_size, &device
                    );

                    let (logits, _) = model.forward(input, None);
                    let logits_flat = logits.reshape([batch_size * seq_length, vocab_size]);
                    let target_flat = target.reshape::<1, _>([batch_size * seq_length]);

                    let loss = loss_fn.forward(logits_flat, target_flat);
                    let loss_val = loss.clone().into_data().as_slice::<f32>().unwrap()[0];
                    
                    let grads = loss.backward();
                    model = model.optimizer_step(&lr_config, &mut optim, grads);

                    // --- DEBUG: Inspeccionar exactamente lo que entra al modelo ---
                    if epoch == 0 && num_batches_processed == 0 {
                        println!("\n--- ğŸ” PRIMER BATCH (TEXTO LIMPIO) ---");
                        for b_idx in 0..2.min(batch_size) {
                            let start = b_idx * stride;
                            let indices = &token_buffer[start..start + seq_length];
                            let decoded = tokenizer.decode(indices);
                            let clean_debug = decoded
                                .replace('â–', " ")
                                .replace('Ä ', " ")
                                .replace('ÄŠ', "\n");
                            
                            println!("SEQ {}:", b_idx);
                            println!("--------------------------------------------------");
                            println!("{}", clean_debug);
                            println!("--------------------------------------------------");
                        }
                        println!("------------------------------------------------------------\n");
                    }

                    total_loss += loss_val;
                    num_batches_processed += 1;

                    // Consumimos los tokens procesados del bÃºfer
                    token_buffer.drain(0..tokens_per_batch);

                    // Reporte fluido
                    let elapsed = batch_start_time.elapsed().as_secs_f32();
                    print!("\r    Batch {} | Loss: {:.4} | Time: {:.3}s | Buf: {}     ", 
                        num_batches_processed, total_loss / num_batches_processed as f32, elapsed, token_buffer.len());
                    io::stdout().flush().unwrap();

                    // --- GUARDADO Y GENERACIÃ“N AUTOMÃTICA ---
                    if last_save_time.elapsed().as_secs() >= 180 {
                        println!("\n\n    ğŸ’¾ Guardado automÃ¡tico (Intervalo 4 min)...");
                        let recorder = CompactRecorder::new();
                        let _ = model.clone().save_file(model_path, &recorder);
                        
                        println!("    ğŸ” Generando muestra de prueba:");
                        let seed = "func _ready():";
                        let generated = generate_text(&model, &tokenizer, seed, 200, vocab_size, &device);
                        
                        let full_text = format!("{}{}", seed, generated);
                        let clean_text = full_text
                            .replace('â–', " ")
                            .replace('Ä ', " ")
                            .replace('ÄŠ', "\n");

                        println!("    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
                        for line in clean_text.lines() {
                            println!("    â”‚ {}", line);
                        }
                        println!("    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
                        
                        last_save_time = Instant::now();
                    }
                }
            }
        }
        println!("\n  âœ… Ã‰poca finalizada. Guardando...");
        let recorder = CompactRecorder::new();
        let _ = model.clone().save_file(model_path, &recorder);
    }

    Ok(())
}
